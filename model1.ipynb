!pip install -q transformers datasets accelerate sentencepiece
from datasets import load_dataset, Dataset
from transformers import MBartForConditionalGeneration, MBartTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import pandas as pd
import torch

from google.colab import files
import json
with open("training_data.json", encoding="utf-8") as f:
    raw_data = json.load(f)
data = []
for item in raw_data:
    source = f"Uzdevums: {item['instruction']}\nStudenta atbilde: {item['input']}"
    target = item["output"]
    data.append({"source": source, "target": target})
from datasets import Dataset
dataset = Dataset.from_list(data)
dataset.shuffle(seed=42).select(range(3))
dataset.shuffle(seed=42).select(range(3)).to_pandas()

from transformers import MBartTokenizer, MBartForConditionalGeneration
LANG_CODE = "lv_LV"
model_name = "facebook/mbart-large-50"
tokenizer = MBartTokenizer.from_pretrained(model_name, src_lang=LANG_CODE, tgt_lang=LANG_CODE)
model = MBartForConditionalGeneration.from_pretrained(model_name)
model.config.forced_bos_token_id = tokenizer.lang_code_to_id[LANG_CODE]
def preprocess(example):
    model_inputs = tokenizer(
        example["source"],
        max_length=512,
        truncation=True,
        padding="longest"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["target"],
            max_length=128,
            truncation=True,
            padding="longest"
        )["input_ids"]
    model_inputs["labels"] = labels
    return model_inputs
tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)
tokenized_dataset = tokenized_dataset.filter(lambda example: any(token > 1 for token in example["labels"]))
tokenized_dataset[0]

tokenized_dataset = tokenized_dataset.filter(
    lambda example: any(token > 1 for token in example["labels"])
)

tokenized_dataset[0]

from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
training_args = TrainingArguments(
    output_dir="./mbart_retrain",
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    num_train_epochs=6,
    weight_decay=0.01,
    save_total_limit=1,
    fp16=True,
    logging_steps=10,
    logging_dir="./logs",
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

def generate_comment_debug(student_text, instruction="Novērtē studenta funkcionālo prasību un sniedz komentāru."):
    input_text = f"Uzdevums: {instruction}\nStudenta atbilde: {student_text}"
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    output_tokens = model.generate(
        **inputs,
        max_length=128,
        num_beams=4,
        forced_bos_token_id=tokenizer.lang_code_to_id["lv_LV"],
        early_stopping=True
    )
    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)

sample_student_answer = "Jānodrošina efektīvā un ātrā veiktspēja (2 minušu laikā), it īpaši, apstrādājot lielus pirkšanas pasūtījumus un piedāvājot lietotājiem labu pieredzi."
generated_comment = generate_comment_debug(sample_student_answer)

from google.colab import files

import json
from datasets import Dataset
with open("training_data.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)
dataset = Dataset.from_list(raw_data)

from transformers import MBartTokenizer, MBartForConditionalGeneration
LANG_CODE = "lv_LV"
model_name = "facebook/mbart-large-50"
tokenizer = MBartTokenizer.from_pretrained(model_name, src_lang=LANG_CODE, tgt_lang=LANG_CODE)
model = MBartForConditionalGeneration.from_pretrained(model_name)
model.config.decoder_start_token_id = tokenizer.lang_code_to_id["lv_LV"]
model.config.forced_bos_token_id = tokenizer.lang_code_to_id[LANG_CODE]
def preprocess(example):
    model_inputs = tokenizer(
        example["input"],
        max_length=512,
        truncation=True,
        padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["output"],
            max_length=128,
            truncation=True,
            padding="max_length"
        )["input_ids"]
    model_inputs["labels"] = labels
    return model_inputs
tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)
tokenized_dataset = tokenized_dataset.filter(lambda example: any(token > 1 for token in example["labels"]))

from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
training_args = TrainingArguments(
    output_dir="./mbart_with_context_model",
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    save_total_limit=1,
    fp16=True,
    logging_steps=10,
    logging_dir="./logs",
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

def generate_comment_with_context_debug(system_summary, student_text):
    input_text = (
        f"Sistēmas apraksts: {system_summary}\n"
        "Vērtēšanas kritēriji: Mērķis, ievaddati, darbības, izvaddati.\n"
        "Uzdevums: Novērtē studentu atbildi, sniedzot komentāru, kas balstās uz iepriekš minēto informāciju.\n"
        f"Studenta atbilde: {student_text}"
    )
    inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).to(model.device)
    output_tokens = model.generate(
        **inputs,
        max_length=128,
        num_beams=4,
        early_stopping=True
    )
    decoded = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
    return decoded

student_text = "Mērķis: Pievienot jauni preci kataloga lai viņus var īre.\nIevaddati: Jaunas preču apraksts  un viņu cena.\nDarbības: Pārbaudīt ir šo prece kataloga vai ne. Ja viņa nav tad pievienot viņu datu bāzei, bet ja viņa ir tad paradīt viņu datu bāzē.\nIzvaddati: Jaunu prece katalogā."
generate_comment_with_context_debug(system_summary, student_text)

!pip install evaluate -q

!pip install -q transformers datasets accelerate sentencepiece evaluate bert-score rouge_score

import evaluate
bertscore = evaluate.load("bertscore")
rouge = evaluate.load("rouge")
results = [
    {"original_comment": "This is the original comment 1.", "generated_comment": "This is the generated comment 1."},
    {"original_comment": "Another original comment.", "generated_comment": "Another generated comment."},
    {"original_comment": "", "generated_comment": "This generated comment has no original."},
    {"original_comment": "Valid original.", "generated_comment": ""},
]
originals = [entry["original_comment"] for entry in results if entry.get("original_comment", "").strip() and entry.get("generated_comment", "").strip()]
generated = [entry["generated_comment"] for entry in results if entry.get("original_comment", "").strip() and entry.get("generated_comment", "").strip()]
if len(originals) == 0:
else:
    bert_result = bertscore.compute(predictions=generated, references=originals, lang="lv")
    rouge_result = rouge.compute(predictions=generated, references=originals)
    for key, value in rouge_result.items():

train_dataset = tokenized_dataset.select(range(140))
eval_dataset = tokenized_dataset.select(range(140, 160))
test_dataset = tokenized_dataset.select(range(160, len(tokenized_dataset)))

from transformers import Seq2SeqTrainingArguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    num_train_epochs=3,
    save_total_limit=2,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_strategy="epoch",
    logging_steps=10,
    report_to="none"
)

from transformers import Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

predictions = trainer.predict(test_dataset)

decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)
decoded_labels = tokenizer.batch_decode(predictions.label_ids, skip_special_tokens=True)

from evaluate import load
rouge = load("rouge")
bertscore = load("bertscore")
rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang="lv")

for key, value in rouge_result.items():

def generate_feedback(example):
    inputs = tokenizer(example["source"], return_tensors="pt", truncation=True, padding="longest").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=128, num_beams=4)
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"source": example["source"], "generated_feedback": decoded_output}

sample_data = dataset.shuffle(seed=42).select(range(5))
feedback_results = [generate_feedback(example) for example in sample_data]



import pandas as pd

results_df = pd.DataFrame(feedback_results)
results_df.to_csv("generated_feedback.csv", index=False, encoding="utf-8")
results_df.head()

def generate_feedback(example):
    inputs = tokenizer(example["source"], return_tensors="pt", truncation=True, padding="longest").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=128, num_beams=4)
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"source": example["source"], "generated_feedback": decoded_output}

sample_data = dataset.shuffle(seed=42).select(range(5))
feedback_results = [generate_feedback(example) for example in sample_data]



import pandas as pd

results_df = pd.DataFrame(feedback_results)
results_df.to_csv("generated_feedback.csv", index=False, encoding="utf-8")
results_df.head()

from google.colab import files
uploaded_test = files.upload()

with open("model_comments.json", encoding="utf-8") as f:
    test_raw = json.load(f)

test_data = []
for item in test_raw:
    source = f"Uzdevums: {item['instruction']}\nStudenta atbilde: {item['input']}"
    test_data.append({"source": source})
def generate_feedback(example):
    inputs = tokenizer(example["source"], return_tensors="pt", truncation=True, padding="longest").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=128, num_beams=4)
    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"source": example["source"], "generated_feedback": decoded_output}
test_feedback_results = [generate_feedback(example) for example in test_data]
import pandas as pd

test_results_df = pd.DataFrame(test_feedback_results)
test_results_df.to_csv("test_feedback_output.csv", index=False, encoding="utf-8")
test_results_df.head()
